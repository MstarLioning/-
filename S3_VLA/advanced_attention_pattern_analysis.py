#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
advanced_attention_pattern_analysis.py
======================================

é«˜çº§æ³¨æ„åŠ›æ¨¡å¼åˆ†æå™¨ - æ·±å…¥åˆ†æé™æ€æ³¨æ„åŠ›æƒé‡çš„ç»“æ„æ€§ç‰¹å¾

ä¸»è¦åŠŸèƒ½ï¼š
1. æ³¨æ„åŠ›æ¨¡å¼è¯†åˆ«ä¸åˆ†ç±»
2. æ³¨æ„åŠ›å¤´ä¸“é—¨åŒ–åˆ†æ  
3. ç©ºé—´ç»“æ„åˆ†æ
4. é«˜çº§é‡åŒ–æŒ‡æ ‡
5. ç»“æ„æ€§å¯è§†åŒ–
6. æˆåŠŸ/å¤±è´¥æ¨¡å¼å¯¹æ¯”
"""

import os
import sys
import numpy as np
import json
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict
import warnings
from datetime import datetime
from scipy import ndimage, sparse
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import pdist, squareform
from sklearn.decomposition import NMF, PCA
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from scipy.stats import entropy
import networkx as nx
from tqdm import tqdm

warnings.filterwarnings('ignore')

plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class AttentionPatternAnalyzer:
    """é«˜çº§æ³¨æ„åŠ›æ¨¡å¼åˆ†æå™¨"""
    
    def __init__(self, results_dir: str):
        self.results_dir = results_dir
        self.analysis_dir = os.path.join(results_dir, "Advanced_Pattern_Analysis")
        os.makedirs(self.analysis_dir, exist_ok=True)
        
        # å­˜å‚¨åˆ†æç»“æœ
        self.pattern_results = {}
        self.structural_metrics = {}
        self.comparative_analysis = {}
        
    def identify_attention_patterns(self, attention_weights: np.ndarray) -> Dict[str, Any]:
        """è¯†åˆ«å…¸å‹çš„æ³¨æ„åŠ›æ¨¡å¼"""
        print("ğŸ” è¯†åˆ«æ³¨æ„åŠ›æ¨¡å¼...")
        
        patterns = {}
        
        # 1. å¯¹è§’çº¿æ¨¡å¼ï¼ˆå±€éƒ¨æ³¨æ„åŠ›ï¼‰
        patterns['diagonal_dominance'] = self._measure_diagonal_dominance(attention_weights)
        
        # 2. å‚ç›´æ¡çº¹æ¨¡å¼ï¼ˆç‰¹å®štokenè¢«å¹¿æ³›å…³æ³¨ï¼‰  
        patterns['vertical_stripes'] = self._find_vertical_stripes(attention_weights)
        
        # 3. æ°´å¹³æ¡çº¹æ¨¡å¼ï¼ˆæŸäº›tokenå…³æ³¨æ‰€æœ‰å…¶ä»–tokenï¼‰
        patterns['horizontal_stripes'] = self._find_horizontal_stripes(attention_weights)
        
        # 4. å—çŠ¶æ¨¡å¼ï¼ˆåˆ†ç»„æ³¨æ„åŠ›ï¼‰
        patterns['block_structures'] = self._detect_block_structures(attention_weights)
        
        # 5. ç¨€ç–æ¨¡å¼ vs å¯†é›†æ¨¡å¼
        patterns['sparsity_structure'] = self._analyze_sparsity_structure(attention_weights)
        
        # 6. å‘¨æœŸæ€§æ¨¡å¼
        patterns['periodicity'] = self._detect_periodic_patterns(attention_weights)
        
        return patterns
    
    def _measure_diagonal_dominance(self, attention_matrix: np.ndarray) -> Dict[str, float]:
        """æµ‹é‡å¯¹è§’çº¿ä¸»å¯¼æ€§"""
        seq_len = attention_matrix.shape[0]
        
        # ä¸»å¯¹è§’çº¿æƒé‡
        main_diagonal = np.diag(attention_matrix).sum()
        
        # ä¸åŒè·ç¦»çš„å¯¹è§’çº¿æƒé‡
        diagonal_weights = {}
        for k in range(1, min(5, seq_len)):
            upper_diag = np.diag(attention_matrix, k=k).sum()
            lower_diag = np.diag(attention_matrix, k=-k).sum()
            diagonal_weights[f'offset_{k}'] = upper_diag + lower_diag
        
        total_weight = attention_matrix.sum()
        main_diagonal_ratio = main_diagonal / total_weight if total_weight > 0 else 0
        
        # å±€éƒ¨æ€§æŒ‡æ ‡ï¼šç›¸é‚»ä½ç½®çš„æ³¨æ„åŠ›å æ¯”
        local_mask = np.abs(np.arange(seq_len)[:, None] - np.arange(seq_len)) <= 2
        local_attention = (attention_matrix * local_mask).sum()
        locality_score = local_attention / total_weight if total_weight > 0 else 0
        
        return {
            'main_diagonal_ratio': float(main_diagonal_ratio),
            'locality_score': float(locality_score),
            'diagonal_weights': diagonal_weights
        }
    
    def _find_vertical_stripes(self, attention_matrix: np.ndarray) -> Dict[str, Any]:
        """æŸ¥æ‰¾å‚ç›´æ¡çº¹æ¨¡å¼ï¼ˆç‰¹å®šä½ç½®è¢«å¹¿æ³›å…³æ³¨ï¼‰"""
        seq_len = attention_matrix.shape[0]
        
        # è®¡ç®—æ¯åˆ—çš„æ–¹å·®ï¼ˆé«˜æ–¹å·®è¡¨ç¤ºä¸å‡åŒ€çš„å…³æ³¨åˆ†å¸ƒï¼‰
        col_variances = np.var(attention_matrix, axis=0)
        
        # è®¡ç®—æ¯åˆ—çš„æœ€å¤§å€¼ï¼ˆè¢«é«˜åº¦å…³æ³¨çš„ä½ç½®ï¼‰
        col_maxes = np.max(attention_matrix, axis=0)
        
        # æŸ¥æ‰¾æ˜¾è‘—çš„å‚ç›´æ¡çº¹
        variance_threshold = np.percentile(col_variances, 90)
        max_threshold = np.percentile(col_maxes, 90)
        
        significant_columns = np.where((col_variances > variance_threshold) | 
                                     (col_maxes > max_threshold))[0]
        
        return {
            'significant_positions': significant_columns.tolist(),
            'col_variances': col_variances.tolist(),
            'col_maxes': col_maxes.tolist(),
            'stripe_count': len(significant_columns)
        }
    
    def _find_horizontal_stripes(self, attention_matrix: np.ndarray) -> Dict[str, Any]:
        """æŸ¥æ‰¾æ°´å¹³æ¡çº¹æ¨¡å¼ï¼ˆæŸäº›tokenå…³æ³¨æ‰€æœ‰ä½ç½®ï¼‰"""
        seq_len = attention_matrix.shape[0]
        
        # è®¡ç®—æ¯è¡Œçš„æ–¹å·®å’Œç†µ
        row_variances = np.var(attention_matrix, axis=1)
        row_entropies = [entropy(row + 1e-8) for row in attention_matrix]
        
        # æŸ¥æ‰¾å…·æœ‰é«˜ç†µï¼ˆå¹¿æ³›å…³æ³¨ï¼‰çš„è¡Œ
        entropy_threshold = np.percentile(row_entropies, 90)
        high_entropy_rows = np.where(np.array(row_entropies) > entropy_threshold)[0]
        
        return {
            'high_attention_sources': high_entropy_rows.tolist(),
            'row_entropies': row_entropies,
            'row_variances': row_variances.tolist(),
            'source_count': len(high_entropy_rows)
        }
    
    def _detect_block_structures(self, attention_matrix: np.ndarray) -> Dict[str, Any]:
        """æ£€æµ‹å—çŠ¶ç»“æ„ï¼ˆåˆ†ç»„æ³¨æ„åŠ›ï¼‰"""
        # ä½¿ç”¨èšç±»æ£€æµ‹æ³¨æ„åŠ›å—
        try:
            # åŸºäºæ³¨æ„åŠ›ç›¸ä¼¼æ€§è¿›è¡Œèšç±»
            similarity_matrix = np.corrcoef(attention_matrix)
            similarity_matrix = np.nan_to_num(similarity_matrix)
            
            # å±‚æ¬¡èšç±»
            condensed_dist = pdist(similarity_matrix, metric='euclidean')
            linkage_matrix = linkage(condensed_dist, method='ward')
            
            # æ£€æµ‹æ˜æ˜¾çš„å—ç»“æ„
            from scipy.cluster.hierarchy import fcluster
            n_clusters = min(5, attention_matrix.shape[0] // 4)
            if n_clusters >= 2:
                clusters = fcluster(linkage_matrix, n_clusters, criterion='maxclust')
                
                # è®¡ç®—å—å†…å’Œå—é—´çš„æ³¨æ„åŠ›å¼ºåº¦
                block_stats = self._compute_block_statistics(attention_matrix, clusters)
                
                return {
                    'n_clusters': n_clusters,
                    'clusters': clusters.tolist(),
                    'block_statistics': block_stats,
                    'linkage_matrix': linkage_matrix.tolist()
                }
            else:
                return {'n_clusters': 0, 'clusters': [], 'block_statistics': {}}
                
        except Exception as e:
            return {'error': str(e), 'n_clusters': 0}
    
    def _compute_block_statistics(self, attention_matrix: np.ndarray, clusters: np.ndarray) -> Dict:
        """è®¡ç®—å—çŠ¶ç»“æ„çš„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        unique_clusters = np.unique(clusters)
        
        intra_block_attention = []
        inter_block_attention = []
        
        for i in range(len(attention_matrix)):
            for j in range(len(attention_matrix)):
                if clusters[i] == clusters[j]:
                    intra_block_attention.append(attention_matrix[i, j])
                else:
                    inter_block_attention.append(attention_matrix[i, j])
        
        stats['intra_block_mean'] = float(np.mean(intra_block_attention)) if intra_block_attention else 0
        stats['inter_block_mean'] = float(np.mean(inter_block_attention)) if inter_block_attention else 0
        stats['block_coherence'] = stats['intra_block_mean'] / (stats['inter_block_mean'] + 1e-8)
        
        return stats
    
    def _analyze_sparsity_structure(self, attention_matrix: np.ndarray) -> Dict[str, Any]:
        """åˆ†æç¨€ç–æ€§ç»“æ„"""
        # è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„ç¨€ç–æ€§
        thresholds = [0.01, 0.05, 0.1, 0.2]
        sparsity_levels = {}
        
        for thresh in thresholds:
            sparse_mask = attention_matrix > thresh
            sparsity = 1 - np.sum(sparse_mask) / attention_matrix.size
            sparsity_levels[f'threshold_{thresh}'] = float(sparsity)
        
        # åˆ†æé«˜æ³¨æ„åŠ›åŒºåŸŸçš„è¿é€šæ€§
        high_attention_mask = attention_matrix > np.percentile(attention_matrix, 90)
        connected_components, num_components = ndimage.label(high_attention_mask)
        
        # è®¡ç®—æœ‰æ•ˆç§©
        try:
            s = np.linalg.svd(attention_matrix, compute_uv=False)
            s_normalized = s / s.sum()
            effective_rank = np.exp(entropy(s_normalized + 1e-8))
        except:
            effective_rank = 0
        
        return {
            'sparsity_levels': sparsity_levels,
            'connected_components': int(num_components),
            'effective_rank': float(effective_rank),
            'rank_ratio': float(effective_rank / min(attention_matrix.shape))
        }
    
    def _detect_periodic_patterns(self, attention_matrix: np.ndarray) -> Dict[str, Any]:
        """æ£€æµ‹å‘¨æœŸæ€§æ¨¡å¼"""
        try:
            # å¯¹æ¯è¡Œè¿›è¡ŒFFTåˆ†æ
            row_periodicities = []
            dominant_frequencies = []
            
            for row in attention_matrix:
                if len(row) > 1:
                    fft = np.fft.fft(row)
                    power_spectrum = np.abs(fft) ** 2
                    
                    # æŸ¥æ‰¾ä¸»å¯¼é¢‘ç‡ï¼ˆæ’é™¤DCåˆ†é‡ï¼‰
                    freqs = np.fft.fftfreq(len(row))
                    non_dc_idx = np.arange(1, len(power_spectrum) // 2)
                    
                    if len(non_dc_idx) > 0:
                        max_freq_idx = non_dc_idx[np.argmax(power_spectrum[non_dc_idx])]
                        dominant_freq = freqs[max_freq_idx]
                        dominant_frequencies.append(dominant_freq)
                        
                        # è®¡ç®—å‘¨æœŸæ€§å¼ºåº¦
                        periodicity_strength = power_spectrum[max_freq_idx] / np.sum(power_spectrum)
                        row_periodicities.append(periodicity_strength)
            
            avg_periodicity = np.mean(row_periodicities) if row_periodicities else 0
            dominant_period = 1 / np.mean(np.abs(dominant_frequencies)) if dominant_frequencies else 0
            
            return {
                'average_periodicity': float(avg_periodicity),
                'dominant_period': float(dominant_period),
                'periodic_strength': float(np.std(row_periodicities)) if row_periodicities else 0
            }
            
        except Exception as e:
            return {'error': str(e), 'average_periodicity': 0}
    
    def compute_structural_metrics(self, attention_weights: np.ndarray) -> Dict[str, Any]:
        """è®¡ç®—ç»“æ„æ€§é‡åŒ–æŒ‡æ ‡"""
        print("ğŸ“Š è®¡ç®—ç»“æ„æ€§æŒ‡æ ‡...")
        
        metrics = {}
        seq_len = attention_weights.shape[0]
        
        # 1. æ³¨æ„åŠ›è·ç¦»åˆ†æ
        metrics['locality'] = self._measure_attention_locality(attention_weights)
        
        # 2. å¯¹ç§°æ€§åˆ†æ
        metrics['symmetry'] = self._measure_attention_symmetry(attention_weights)
        
        # 3. æ–¹å‘æ€§åˆ†æ
        metrics['directionality'] = self._measure_attention_directionality(attention_weights)
        
        # 4. å¤æ‚åº¦æŒ‡æ ‡
        metrics['complexity'] = self._measure_pattern_complexity(attention_weights)
        
        # 5. é›†ä¸­åº¦æŒ‡æ ‡
        metrics['concentration'] = self._measure_attention_concentration(attention_weights)
        
        return metrics
    
    def _measure_attention_locality(self, attention_matrix: np.ndarray) -> Dict[str, float]:
        """æµ‹é‡æ³¨æ„åŠ›çš„å±€éƒ¨æ€§"""
        seq_len = attention_matrix.shape[0]
        
        # è®¡ç®—å¹³å‡æ³¨æ„åŠ›è·ç¦»
        distances = []
        weights = []
        
        for i in range(seq_len):
            for j in range(seq_len):
                distances.append(abs(i - j))
                weights.append(attention_matrix[i, j])
        
        avg_distance = np.average(distances, weights=weights) if np.sum(weights) > 0 else 0
        
        # å±€éƒ¨çª—å£å†…çš„æ³¨æ„åŠ›æ¯”ä¾‹
        local_windows = [1, 2, 3, 5]
        local_ratios = {}
        
        for window in local_windows:
            local_mask = np.abs(np.arange(seq_len)[:, None] - np.arange(seq_len)) <= window
            local_attention = (attention_matrix * local_mask).sum()
            total_attention = attention_matrix.sum()
            local_ratios[f'window_{window}'] = float(local_attention / total_attention) if total_attention > 0 else 0
        
        return {
            'average_distance': float(avg_distance),
            'local_ratios': local_ratios
        }
    
    def _measure_attention_symmetry(self, attention_matrix: np.ndarray) -> Dict[str, float]:
        """æµ‹é‡æ³¨æ„åŠ›çš„å¯¹ç§°æ€§"""
        # è®¡ç®—çŸ©é˜µä¸å…¶è½¬ç½®çš„ç›¸ä¼¼åº¦
        symmetry_score = np.corrcoef(attention_matrix.flatten(), attention_matrix.T.flatten())[0, 1]
        if np.isnan(symmetry_score):
            symmetry_score = 0
        
        # è®¡ç®—åå¯¹è§’çº¿å¯¹ç§°æ€§
        flipped = np.fliplr(attention_matrix)
        anti_symmetry = np.corrcoef(attention_matrix.flatten(), flipped.flatten())[0, 1]
        if np.isnan(anti_symmetry):
            anti_symmetry = 0
        
        return {
            'matrix_symmetry': float(symmetry_score),
            'anti_symmetry': float(anti_symmetry)
        }
    
    def _measure_attention_directionality(self, attention_matrix: np.ndarray) -> Dict[str, float]:
        """æµ‹é‡æ³¨æ„åŠ›çš„æ–¹å‘æ€§"""
        # å‰å‘vsåå‘æ³¨æ„åŠ›
        upper_triangular = np.triu(attention_matrix, k=1).sum()
        lower_triangular = np.tril(attention_matrix, k=-1).sum()
        
        total_off_diagonal = upper_triangular + lower_triangular
        
        if total_off_diagonal > 0:
            forward_bias = (upper_triangular - lower_triangular) / total_off_diagonal
        else:
            forward_bias = 0
        
        return {
            'forward_bias': float(forward_bias),
            'forward_ratio': float(upper_triangular / total_off_diagonal) if total_off_diagonal > 0 else 0,
            'backward_ratio': float(lower_triangular / total_off_diagonal) if total_off_diagonal > 0 else 0
        }
    
    def _measure_pattern_complexity(self, attention_matrix: np.ndarray) -> Dict[str, float]:
        """æµ‹é‡æ¨¡å¼å¤æ‚åº¦"""
        # åŸºäºSVDçš„å¤æ‚åº¦
        try:
            U, s, Vt = np.linalg.svd(attention_matrix)
            
            # æœ‰æ•ˆç§©
            s_normalized = s / s.sum() if s.sum() > 0 else s
            effective_rank = np.exp(entropy(s_normalized + 1e-8))
            
            # å¥‡å¼‚å€¼é›†ä¸­åº¦
            cumsum_s = np.cumsum(s_normalized)
            rank_90 = np.argmax(cumsum_s >= 0.9) + 1
            
            # ç»“æ„ç†µ
            structure_entropy = entropy(attention_matrix.flatten() + 1e-8)
            
            return {
                'effective_rank': float(effective_rank),
                'rank_90_percent': int(rank_90),
                'structure_entropy': float(structure_entropy),
                'singular_value_ratio': float(s[0] / s.sum()) if len(s) > 0 and s.sum() > 0 else 0
            }
            
        except Exception as e:
            return {
                'effective_rank': 0,
                'rank_90_percent': 0,
                'structure_entropy': 0,
                'error': str(e)
            }
    
    def _measure_attention_concentration(self, attention_matrix: np.ndarray) -> Dict[str, float]:
        """æµ‹é‡æ³¨æ„åŠ›é›†ä¸­åº¦"""
        # Giniç³»æ•°
        def gini_coefficient(x):
            x = np.sort(x.flatten())
            n = len(x)
            cumsum = np.cumsum(x)
            return (n + 1 - 2 * np.sum(cumsum) / cumsum[-1]) / n if cumsum[-1] > 0 else 0
        
        gini = gini_coefficient(attention_matrix)
        
        # æœ€å¤§å€¼æ¯”ä¾‹
        max_attention = np.max(attention_matrix)
        total_attention = np.sum(attention_matrix)
        max_ratio = max_attention / total_attention if total_attention > 0 else 0
        
        # Top-Kæ³¨æ„åŠ›é›†ä¸­åº¦
        flat_attention = attention_matrix.flatten()
        sorted_attention = np.sort(flat_attention)[::-1]
        
        top_k_ratios = {}
        for k in [1, 5, 10, 20]:
            if k <= len(sorted_attention):
                top_k_sum = np.sum(sorted_attention[:k])
                top_k_ratios[f'top_{k}'] = float(top_k_sum / total_attention) if total_attention > 0 else 0
        
        return {
            'gini_coefficient': float(gini),
            'max_ratio': float(max_ratio),
            'top_k_ratios': top_k_ratios
        }
    
    def create_pattern_visualizations(self, attention_data: Dict, task_name: str, success_status: str):
        """åˆ›å»ºæ³¨æ„åŠ›æ¨¡å¼çš„é«˜çº§å¯è§†åŒ–"""
        print(f"ğŸ¨ ä¸º {task_name}_{success_status} åˆ›å»ºé«˜çº§å¯è§†åŒ–...")
        
        viz_dir = os.path.join(self.analysis_dir, f"{task_name}_{success_status}_patterns")
        os.makedirs(viz_dir, exist_ok=True)
        
        # 1. æ³¨æ„åŠ›æ¨¡å¼çƒ­å›¾é›†åˆ
        self._create_pattern_heatmaps(attention_data, viz_dir, f"{task_name}_{success_status}")
        
        # 2. ç»“æ„åˆ†æå›¾è¡¨
        self._create_structural_analysis_plots(attention_data, viz_dir, f"{task_name}_{success_status}")
        
        # 3. ç½‘ç»œå›¾å¯è§†åŒ–
        self._create_network_visualizations(attention_data, viz_dir, f"{task_name}_{success_status}")
        
        print(f"âœ… {task_name}_{success_status} å¯è§†åŒ–å®Œæˆ")
    
    def _create_pattern_heatmaps(self, attention_data: Dict, viz_dir: str, prefix: str):
        """åˆ›å»ºæ¨¡å¼çƒ­å›¾"""
        patterns = attention_data.get('patterns', {})
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'Attention Pattern Analysis: {prefix}', fontsize=16)
        
        # 1. åŸå§‹æ³¨æ„åŠ›çƒ­å›¾ï¼ˆå¦‚æœæœ‰æ ·æœ¬æ•°æ®ï¼‰
        if 'sample_attention_matrix' in attention_data:
            sample_matrix = np.array(attention_data['sample_attention_matrix'])
            im1 = axes[0, 0].imshow(sample_matrix, cmap='YlOrRd', aspect='auto')
            axes[0, 0].set_title('Sample Attention Matrix')
            axes[0, 0].set_xlabel('Key Position')
            axes[0, 0].set_ylabel('Query Position')
            plt.colorbar(im1, ax=axes[0, 0])
        
        # 2. ç¨€ç–æ€§ç»“æ„
        sparsity_data = patterns.get('sparsity_structure', {})
        sparsity_levels = sparsity_data.get('sparsity_levels', {})
        if sparsity_levels:
            thresholds = list(sparsity_levels.keys())
            sparsity_values = list(sparsity_levels.values())
            axes[0, 1].bar(range(len(thresholds)), sparsity_values, alpha=0.7)
            axes[0, 1].set_title('Sparsity at Different Thresholds')
            axes[0, 1].set_xticks(range(len(thresholds)))
            axes[0, 1].set_xticklabels([t.replace('threshold_', '') for t in thresholds])
            axes[0, 1].set_ylabel('Sparsity Ratio')
        
        # 3. å¯¹è§’çº¿ä¸»å¯¼æ€§
        diagonal_data = patterns.get('diagonal_dominance', {})
        if diagonal_data:
            locality_score = diagonal_data.get('locality_score', 0)
            main_diagonal_ratio = diagonal_data.get('main_diagonal_ratio', 0)
            
            metrics = ['Locality', 'Main Diagonal']
            values = [locality_score, main_diagonal_ratio]
            axes[0, 2].bar(metrics, values, alpha=0.7, color=['blue', 'orange'])
            axes[0, 2].set_title('Locality Metrics')
            axes[0, 2].set_ylabel('Score')
        
        # 4. å‚ç›´æ¡çº¹åˆ†æ
        vertical_data = patterns.get('vertical_stripes', {})
        if vertical_data and 'col_variances' in vertical_data:
            col_variances = vertical_data['col_variances']
            axes[1, 0].plot(col_variances, alpha=0.7, color='green')
            axes[1, 0].set_title('Column Variance (Vertical Stripes)')
            axes[1, 0].set_xlabel('Position')
            axes[1, 0].set_ylabel('Variance')
            axes[1, 0].grid(True, alpha=0.3)
        
        # 5. æ°´å¹³æ¡çº¹åˆ†æ
        horizontal_data = patterns.get('horizontal_stripes', {})
        if horizontal_data and 'row_entropies' in horizontal_data:
            row_entropies = horizontal_data['row_entropies']
            axes[1, 1].plot(row_entropies, alpha=0.7, color='red')
            axes[1, 1].set_title('Row Entropy (Horizontal Stripes)')
            axes[1, 1].set_xlabel('Position')
            axes[1, 1].set_ylabel('Entropy')
            axes[1, 1].grid(True, alpha=0.3)
        
        # 6. å‘¨æœŸæ€§åˆ†æ
        periodic_data = patterns.get('periodicity', {})
        if periodic_data:
            avg_periodicity = periodic_data.get('average_periodicity', 0)
            dominant_period = periodic_data.get('dominant_period', 0)
            
            metrics = ['Avg Periodicity', 'Dominant Period']
            values = [avg_periodicity, dominant_period]
            axes[1, 2].bar(metrics, values, alpha=0.7, color=['purple', 'brown'])
            axes[1, 2].set_title('Periodicity Analysis')
            axes[1, 2].set_ylabel('Score')
        
        plt.tight_layout()
        save_path = os.path.join(viz_dir, f"{prefix}_pattern_heatmaps.png")
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
    
    def _create_structural_analysis_plots(self, attention_data: Dict, viz_dir: str, prefix: str):
        """åˆ›å»ºç»“æ„åˆ†æå›¾è¡¨"""
        metrics = attention_data.get('structural_metrics', {})
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f'Structural Analysis: {prefix}', fontsize=16)
        
        # 1. å±€éƒ¨æ€§åˆ†æ
        locality_data = metrics.get('locality', {})
        if locality_data and 'local_ratios' in locality_data:
            local_ratios = locality_data['local_ratios']
            windows = list(local_ratios.keys())
            ratios = list(local_ratios.values())
            
            axes[0, 0].bar(range(len(windows)), ratios, alpha=0.7)
            axes[0, 0].set_title('Local Attention Ratios')
            axes[0, 0].set_xticks(range(len(windows)))
            axes[0, 0].set_xticklabels([w.replace('window_', 'W') for w in windows])
            axes[0, 0].set_ylabel('Attention Ratio')
        
        # 2. æ–¹å‘æ€§åˆ†æ
        directionality_data = metrics.get('directionality', {})
        if directionality_data:
            forward_ratio = directionality_data.get('forward_ratio', 0)
            backward_ratio = directionality_data.get('backward_ratio', 0)
            
            labels = ['Forward', 'Backward']
            values = [forward_ratio, backward_ratio]
            colors = ['lightblue', 'lightcoral']
            
            axes[0, 1].pie(values, labels=labels, colors=colors, autopct='%1.1f%%')
            axes[0, 1].set_title('Attention Directionality')
        
        # 3. å¤æ‚åº¦æŒ‡æ ‡
        complexity_data = metrics.get('complexity', {})
        if complexity_data:
            effective_rank = complexity_data.get('effective_rank', 0)
            structure_entropy = complexity_data.get('structure_entropy', 0)
            
            metrics_names = ['Effective Rank', 'Structure Entropy']
            metrics_values = [effective_rank, structure_entropy]
            
            axes[1, 0].bar(metrics_names, metrics_values, alpha=0.7, color=['gold', 'silver'])
            axes[1, 0].set_title('Complexity Metrics')
            axes[1, 0].set_ylabel('Value')
        
        # 4. é›†ä¸­åº¦åˆ†æ
        concentration_data = metrics.get('concentration', {})
        if concentration_data and 'top_k_ratios' in concentration_data:
            top_k_ratios = concentration_data['top_k_ratios']
            k_values = list(top_k_ratios.keys())
            k_ratios = list(top_k_ratios.values())
            
            axes[1, 1].plot(range(len(k_values)), k_ratios, 'o-', alpha=0.7)
            axes[1, 1].set_title('Top-K Attention Concentration')
            axes[1, 1].set_xticks(range(len(k_values)))
            axes[1, 1].set_xticklabels([k.replace('top_', '') for k in k_values])
            axes[1, 1].set_xlabel('Top K')
            axes[1, 1].set_ylabel('Attention Ratio')
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        save_path = os.path.join(viz_dir, f"{prefix}_structural_analysis.png")
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
    
    def _create_network_visualizations(self, attention_data: Dict, viz_dir: str, prefix: str):
        """åˆ›å»ºç½‘ç»œå›¾å¯è§†åŒ–"""
        # è¿™é‡Œåˆ›å»ºä¸€ä¸ªæ¦‚å¿µæ€§çš„ç½‘ç»œå¯è§†åŒ–
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        fig.suptitle(f'Network Analysis: {prefix}', fontsize=16)
        
        # 1. æ³¨æ„åŠ›æµå‘å›¾ï¼ˆæ¦‚å¿µæ€§ï¼‰
        patterns = attention_data.get('patterns', {})
        vertical_stripes = patterns.get('vertical_stripes', {})
        
        if 'significant_positions' in vertical_stripes:
            significant_pos = vertical_stripes['significant_positions']
            
            # åˆ›å»ºæ³¨æ„åŠ›hubå›¾
            if significant_pos:
                pos_counts = {}
                for pos in significant_pos:
                    pos_counts[pos] = pos_counts.get(pos, 0) + 1
                
                positions = list(pos_counts.keys())
                counts = list(pos_counts.values())
                
                axes[0].scatter(positions, counts, s=100, alpha=0.7, c='red')
                axes[0].set_title('Attention Hubs (Highly Attended Positions)')
                axes[0].set_xlabel('Position')
                axes[0].set_ylabel('Attention Count')
                axes[0].grid(True, alpha=0.3)
        
        # 2. å—ç»“æ„å¯è§†åŒ–
        block_data = patterns.get('block_structures', {})
        if 'clusters' in block_data and block_data['clusters']:
            clusters = block_data['clusters']
            n_clusters = len(set(clusters))
            
            # åˆ›å»ºèšç±»å¯è§†åŒ–
            cluster_sizes = [clusters.count(i) for i in range(n_clusters)]
            
            axes[1].bar(range(n_clusters), cluster_sizes, alpha=0.7)
            axes[1].set_title('Block Structure (Cluster Sizes)')
            axes[1].set_xlabel('Cluster ID')
            axes[1].set_ylabel('Cluster Size')
        
        plt.tight_layout()
        save_path = os.path.join(viz_dir, f"{prefix}_network_analysis.png")
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
    
    def compare_success_failure_patterns(self, success_data: Dict, failure_data: Dict, task_name: str):
        """æ¯”è¾ƒæˆåŠŸå’Œå¤±è´¥æ¡ˆä¾‹çš„æ³¨æ„åŠ›æ¨¡å¼"""
        print(f"ğŸ”¬ æ¯”è¾ƒ {task_name} çš„æˆåŠŸ/å¤±è´¥æ¨¡å¼...")
        
        comparison_dir = os.path.join(self.analysis_dir, f"{task_name}_success_failure_comparison")
        os.makedirs(comparison_dir, exist_ok=True)
        
        # åˆ›å»ºå¯¹æ¯”å›¾
        self._create_comparison_charts(success_data, failure_data, comparison_dir, task_name)
        
        # è®¡ç®—å·®å¼‚æŒ‡æ ‡
        differences = self._compute_pattern_differences(success_data, failure_data)
        
        # ä¿å­˜å·®å¼‚åˆ†æ
        diff_path = os.path.join(comparison_dir, f"{task_name}_pattern_differences.json")
        with open(diff_path, 'w', encoding='utf-8') as f:
            json.dump(differences, f, indent=2, ensure_ascii=False)
        
        return differences
    
    def _create_comparison_charts(self, success_data: Dict, failure_data: Dict, 
                                 comparison_dir: str, task_name: str):
        """åˆ›å»ºæˆåŠŸ/å¤±è´¥å¯¹æ¯”å›¾è¡¨"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'Success vs Failure Pattern Comparison: {task_name}', fontsize=16)
        
        # 1. å±€éƒ¨æ€§å¯¹æ¯”
        self._plot_locality_comparison(success_data, failure_data, axes[0, 0])
        
        # 2. å¤æ‚åº¦å¯¹æ¯”
        self._plot_complexity_comparison(success_data, failure_data, axes[0, 1])
        
        # 3. æ–¹å‘æ€§å¯¹æ¯”
        self._plot_directionality_comparison(success_data, failure_data, axes[0, 2])
        
        # 4. ç¨€ç–æ€§å¯¹æ¯”
        self._plot_sparsity_comparison(success_data, failure_data, axes[1, 0])
        
        # 5. é›†ä¸­åº¦å¯¹æ¯”
        self._plot_concentration_comparison(success_data, failure_data, axes[1, 1])
        
        # 6. å‘¨æœŸæ€§å¯¹æ¯”
        self._plot_periodicity_comparison(success_data, failure_data, axes[1, 2])
        
        plt.tight_layout()
        save_path = os.path.join(comparison_dir, f"{task_name}_comparison_charts.png")
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
    
    def _plot_locality_comparison(self, success_data: Dict, failure_data: Dict, ax):
        """ç»˜åˆ¶å±€éƒ¨æ€§å¯¹æ¯”"""
        success_locality = success_data.get('structural_metrics', {}).get('locality', {})
        failure_locality = failure_data.get('structural_metrics', {}).get('locality', {})
        
        success_avg_dist = success_locality.get('average_distance', 0)
        failure_avg_dist = failure_locality.get('average_distance', 0)
        
        categories = ['Success', 'Failure']
        values = [success_avg_dist, failure_avg_dist]
        colors = ['green', 'red']
        
        bars = ax.bar(categories, values, color=colors, alpha=0.7)
        ax.set_title('Average Attention Distance')
        ax.set_ylabel('Distance')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,
                   f'{value:.3f}', ha='center', va='bottom')
    
    def _plot_complexity_comparison(self, success_data: Dict, failure_data: Dict, ax):
        """ç»˜åˆ¶å¤æ‚åº¦å¯¹æ¯”"""
        success_complexity = success_data.get('structural_metrics', {}).get('complexity', {})
        failure_complexity = failure_data.get('structural_metrics', {}).get('complexity', {})
        
        success_rank = success_complexity.get('effective_rank', 0)
        failure_rank = failure_complexity.get('effective_rank', 0)
        
        categories = ['Success', 'Failure']
        values = [success_rank, failure_rank]
        colors = ['green', 'red']
        
        bars = ax.bar(categories, values, color=colors, alpha=0.7)
        ax.set_title('Effective Rank (Complexity)')
        ax.set_ylabel('Effective Rank')
        
        for bar, value in zip(bars, values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,
                   f'{value:.2f}', ha='center', va='bottom')
    
    def _plot_directionality_comparison(self, success_data: Dict, failure_data: Dict, ax):
        """ç»˜åˆ¶æ–¹å‘æ€§å¯¹æ¯”"""
        success_dir = success_data.get('structural_metrics', {}).get('directionality', {})
        failure_dir = failure_data.get('structural_metrics', {}).get('directionality', {})
        
        success_bias = success_dir.get('forward_bias', 0)
        failure_bias = failure_dir.get('forward_bias', 0)
        
        categories = ['Success', 'Failure']
        values = [success_bias, failure_bias]
        colors = ['green', 'red']
        
        bars = ax.bar(categories, values, color=colors, alpha=0.7)
        ax.set_title('Forward Bias')
        ax.set_ylabel('Bias Score')
        ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)
        
        for bar, value in zip(bars, values):
            y_pos = bar.get_height() + (max(values) - min(values))*0.01 if value >= 0 else bar.get_height() - (max(values) - min(values))*0.01
            ax.text(bar.get_x() + bar.get_width()/2, y_pos,
                   f'{value:.3f}', ha='center', va='bottom' if value >= 0 else 'top')
    
    def _plot_sparsity_comparison(self, success_data: Dict, failure_data: Dict, ax):
        """ç»˜åˆ¶ç¨€ç–æ€§å¯¹æ¯”"""
        success_sparsity = success_data.get('patterns', {}).get('sparsity_structure', {})
        failure_sparsity = failure_data.get('patterns', {}).get('sparsity_structure', {})
        
        success_rank_ratio = success_sparsity.get('rank_ratio', 0)
        failure_rank_ratio = failure_sparsity.get('rank_ratio', 0)
        
        categories = ['Success', 'Failure']
        values = [success_rank_ratio, failure_rank_ratio]
        colors = ['green', 'red']
        
        bars = ax.bar(categories, values, color=colors, alpha=0.7)
        ax.set_title('Rank Ratio (Sparsity)')
        ax.set_ylabel('Rank Ratio')
        
        for bar, value in zip(bars, values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,
                   f'{value:.3f}', ha='center', va='bottom')
    
    def _plot_concentration_comparison(self, success_data: Dict, failure_data: Dict, ax):
        """ç»˜åˆ¶é›†ä¸­åº¦å¯¹æ¯”"""
        success_conc = success_data.get('structural_metrics', {}).get('concentration', {})
        failure_conc = failure_data.get('structural_metrics', {}).get('concentration', {})
        
        success_gini = success_conc.get('gini_coefficient', 0)
        failure_gini = failure_conc.get('gini_coefficient', 0)
        
        categories = ['Success', 'Failure']
        values = [success_gini, failure_gini]
        colors = ['green', 'red']
        
        bars = ax.bar(categories, values, color=colors, alpha=0.7)
        ax.set_title('Gini Coefficient (Concentration)')
        ax.set_ylabel('Gini Coefficient')
        
        for bar, value in zip(bars, values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,
                   f'{value:.3f}', ha='center', va='bottom')
    
    def _plot_periodicity_comparison(self, success_data: Dict, failure_data: Dict, ax):
        """ç»˜åˆ¶å‘¨æœŸæ€§å¯¹æ¯”"""
        success_period = success_data.get('patterns', {}).get('periodicity', {})
        failure_period = failure_data.get('patterns', {}).get('periodicity', {})
        
        success_avg = success_period.get('average_periodicity', 0)
        failure_avg = failure_period.get('average_periodicity', 0)
        
        categories = ['Success', 'Failure']
        values = [success_avg, failure_avg]
        colors = ['green', 'red']
        
        bars = ax.bar(categories, values, color=colors, alpha=0.7)
        ax.set_title('Average Periodicity')
        ax.set_ylabel('Periodicity Score')
        
        for bar, value in zip(bars, values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,
                   f'{value:.3f}', ha='center', va='bottom')
    
    def _compute_pattern_differences(self, success_data: Dict, failure_data: Dict) -> Dict:
        """è®¡ç®—æ¨¡å¼å·®å¼‚æŒ‡æ ‡"""
        differences = {}
        
        # ç»“æ„æŒ‡æ ‡å·®å¼‚
        success_metrics = success_data.get('structural_metrics', {})
        failure_metrics = failure_data.get('structural_metrics', {})
        
        # å±€éƒ¨æ€§å·®å¼‚
        success_locality = success_metrics.get('locality', {}).get('average_distance', 0)
        failure_locality = failure_metrics.get('locality', {}).get('average_distance', 0)
        differences['locality_difference'] = abs(success_locality - failure_locality)
        
        # å¤æ‚åº¦å·®å¼‚
        success_complexity = success_metrics.get('complexity', {}).get('effective_rank', 0)
        failure_complexity = failure_metrics.get('complexity', {}).get('effective_rank', 0)
        differences['complexity_difference'] = abs(success_complexity - failure_complexity)
        
        # æ–¹å‘æ€§å·®å¼‚
        success_bias = success_metrics.get('directionality', {}).get('forward_bias', 0)
        failure_bias = failure_metrics.get('directionality', {}).get('forward_bias', 0)
        differences['directionality_difference'] = abs(success_bias - failure_bias)
        
        # é›†ä¸­åº¦å·®å¼‚
        success_gini = success_metrics.get('concentration', {}).get('gini_coefficient', 0)
        failure_gini = failure_metrics.get('concentration', {}).get('gini_coefficient', 0)
        differences['concentration_difference'] = abs(success_gini - failure_gini)
        
        # è®¡ç®—æ€»ä½“å·®å¼‚å¾—åˆ†
        all_diffs = [differences['locality_difference'], differences['complexity_difference'],
                    differences['directionality_difference'], differences['concentration_difference']]
        differences['overall_difference_score'] = np.mean(all_diffs)
        
        return differences
    
    def analyze_existing_results(self, results_dir: str):
        """åˆ†æå·²æœ‰çš„Fixed_Attention_Resultsæ•°æ®"""
        print("ğŸ” åˆ†æå·²æœ‰çš„æ³¨æ„åŠ›åˆ†æç»“æœ...")
        
        fixed_results_dir = os.path.join(results_dir, "Fixed_Attention_Results")
        if not os.path.exists(fixed_results_dir):
            print(f"âŒ æœªæ‰¾åˆ°ç»“æœç›®å½•: {fixed_results_dir}")
            return
        
        # è¯»å–æ‰€æœ‰ä»»åŠ¡çš„åˆ†æç»“æœ
        task_analyses = {}
        
        for file in os.listdir(fixed_results_dir):
            if file.endswith('_attention_analysis.json'):
                task_name = file.replace('_attention_analysis.json', '')
                file_path = os.path.join(fixed_results_dir, file)
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        task_data = json.load(f)
                    task_analyses[task_name] = task_data
                    print(f"  âœ… åŠ è½½ä»»åŠ¡: {task_name}")
                except Exception as e:
                    print(f"  âŒ åŠ è½½å¤±è´¥ {task_name}: {e}")
        
        # å¯¹æ¯ä¸ªä»»åŠ¡è¿›è¡Œé«˜çº§æ¨¡å¼åˆ†æ
        all_results = {}
        
        for task_name, task_data in task_analyses.items():
            print(f"\nğŸ“Š åˆ†æä»»åŠ¡: {task_name}")
            
            # åˆ†ææˆåŠŸå’Œå¤±è´¥æ¡ˆä¾‹
            success_analysis = self._analyze_task_patterns(task_data, 'success')
            failure_analysis = self._analyze_task_patterns(task_data, 'failure')
            
            if success_analysis:
                self.create_pattern_visualizations(success_analysis, task_name, 'success')
            
            if failure_analysis:
                self.create_pattern_visualizations(failure_analysis, task_name, 'failure')
            
            # æ¯”è¾ƒåˆ†æ
            if success_analysis and failure_analysis:
                comparison_results = self.compare_success_failure_patterns(
                    success_analysis, failure_analysis, task_name
                )
                all_results[task_name] = {
                    'success_patterns': success_analysis,
                    'failure_patterns': failure_analysis,
                    'comparison': comparison_results
                }
            
        # ä¿å­˜ç»¼åˆåˆ†æç»“æœ
        comprehensive_path = os.path.join(self.analysis_dir, "comprehensive_pattern_analysis.json")
        with open(comprehensive_path, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        
        # åˆ›å»ºè·¨ä»»åŠ¡æ±‡æ€»
        self._create_cross_task_summary(all_results)
        
        print(f"\nğŸ‰ é«˜çº§æ¨¡å¼åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {self.analysis_dir}")
    
    def _analyze_task_patterns(self, task_data: Dict, status_type: str) -> Optional[Dict]:
        """åˆ†æç‰¹å®šçŠ¶æ€çš„ä»»åŠ¡æ¨¡å¼"""
        attention_analysis = task_data.get('attention_analysis', {})
        frames_data = attention_analysis.get(f'{status_type}_frames', {})
        
        if not frames_data or not frames_data.get('layer_summaries'):
            return None
        
        # ä»layer_summariesæ„é€ è™šæ‹Ÿæ³¨æ„åŠ›çŸ©é˜µè¿›è¡Œåˆ†æ
        layer_summaries = frames_data['layer_summaries']
        
        # é€‰æ‹©ä¸€ä¸ªæœ‰ä»£è¡¨æ€§çš„å±‚è¿›è¡Œè¯¦ç»†åˆ†æ
        if not layer_summaries:
            return None
        
        # å–ç¬¬ä¸€ä¸ªæœ‰æ•ˆå±‚çš„æ•°æ®
        representative_layer = list(layer_summaries.keys())[0]
        layer_stats = layer_summaries[representative_layer]
        
        # ç”ŸæˆåŸºäºç»Ÿè®¡ä¿¡æ¯çš„åˆæˆæ³¨æ„åŠ›çŸ©é˜µ
        matrix_size = 64  # å‡è®¾64x64çš„æ³¨æ„åŠ›çŸ©é˜µ
        synthetic_matrix = self._generate_synthetic_attention_matrix(layer_stats, matrix_size)
        
        # è¿›è¡Œæ¨¡å¼åˆ†æ
        patterns = self.identify_attention_patterns(synthetic_matrix)
        structural_metrics = self.compute_structural_metrics(synthetic_matrix)
        
        return {
            'patterns': patterns,
            'structural_metrics': structural_metrics,
            'sample_attention_matrix': synthetic_matrix.tolist(),
            'layer_count': len(layer_summaries),
            'frame_count': frames_data.get('frame_count', 0)
        }
    
    def _generate_synthetic_attention_matrix(self, layer_stats: Dict, size: int) -> np.ndarray:
        """åŸºäºç»Ÿè®¡ä¿¡æ¯ç”Ÿæˆåˆæˆæ³¨æ„åŠ›çŸ©é˜µ"""
        # ä»ç»Ÿè®¡ä¿¡æ¯é‡æ„æ³¨æ„åŠ›çŸ©é˜µ
        mean_val = layer_stats.get('avg_mean', 0.1)
        std_val = layer_stats.get('avg_std', 0.05)
        max_val = layer_stats.get('avg_max', 0.8)
        
        # ç”ŸæˆåŸºç¡€çŸ©é˜µ
        matrix = np.random.normal(mean_val, std_val, (size, size))
        matrix = np.abs(matrix)  # ç¡®ä¿éè´Ÿ
        
        # æ·»åŠ ä¸€äº›ç»“æ„æ€§ç‰¹å¾
        # 1. å¯¹è§’çº¿å¢å¼ºï¼ˆå±€éƒ¨æ³¨æ„åŠ›ï¼‰
        for i in range(size):
            for j in range(size):
                if abs(i - j) <= 2:
                    matrix[i, j] *= 1.5
        
        # 2. éšæœºè®¾ç½®ä¸€äº›é«˜æ³¨æ„åŠ›ç‚¹
        n_peaks = max(1, int(size * 0.1))
        peak_positions = np.random.choice(size*size, n_peaks, replace=False)
        for pos in peak_positions:
            i, j = pos // size, pos % size
            matrix[i, j] = max_val * np.random.uniform(0.8, 1.0)
        
        # 3. å½’ä¸€åŒ–æ¯è¡Œ
        row_sums = matrix.sum(axis=1, keepdims=True)
        matrix = matrix / (row_sums + 1e-8)
        
        return matrix
    
    def _create_cross_task_summary(self, all_results: Dict):
        """åˆ›å»ºè·¨ä»»åŠ¡æ±‡æ€»åˆ†æ"""
        print("ğŸ“ˆ åˆ›å»ºè·¨ä»»åŠ¡æ±‡æ€»åˆ†æ...")
        
        summary_data = {
            'task_count': len(all_results),
            'tasks_analyzed': list(all_results.keys()),
            'cross_task_patterns': {},
            'significant_differences': []
        }
        
        # æ”¶é›†æ‰€æœ‰ä»»åŠ¡çš„å·®å¼‚æŒ‡æ ‡
        all_differences = []
        task_difference_scores = {}
        
        for task_name, task_results in all_results.items():
            comparison = task_results.get('comparison', {})
            overall_diff = comparison.get('overall_difference_score', 0)
            
            if overall_diff > 0:
                all_differences.append(overall_diff)
                task_difference_scores[task_name] = overall_diff
        
        # è¯†åˆ«å·®å¼‚æ˜¾è‘—çš„ä»»åŠ¡
        if all_differences:
            diff_threshold = np.percentile(all_differences, 75)
            significant_tasks = {k: v for k, v in task_difference_scores.items() if v > diff_threshold}
            summary_data['significant_differences'] = significant_tasks
        
        # åˆ›å»ºæ±‡æ€»å¯è§†åŒ–
        self._create_summary_visualizations(all_results, summary_data)
        
        # ä¿å­˜æ±‡æ€»æ•°æ®
        summary_path = os.path.join(self.analysis_dir, "cross_task_pattern_summary.json")
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
    
    def _create_summary_visualizations(self, all_results: Dict, summary_data: Dict):
        """åˆ›å»ºæ±‡æ€»å¯è§†åŒ–"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Cross-Task Pattern Analysis Summary', fontsize=16)
        
        # 1. ä»»åŠ¡å·®å¼‚å¾—åˆ†æ’å
        significant_diffs = summary_data.get('significant_differences', {})
        if significant_diffs:
            tasks = list(significant_diffs.keys())
            scores = list(significant_diffs.values())
            
            y_pos = np.arange(len(tasks))
            axes[0, 0].barh(y_pos, scores, alpha=0.7)
            axes[0, 0].set_yticks(y_pos)
            axes[0, 0].set_yticklabels([t.replace('google_robot_', '') for t in tasks])
            axes[0, 0].set_xlabel('Difference Score')
            axes[0, 0].set_title('Task Pattern Difference Ranking')
        
        # 2. å±€éƒ¨æ€§å¯¹æ¯”
        success_localities = []
        failure_localities = []
        task_names = []
        
        for task_name, task_results in all_results.items():
            success_patterns = task_results.get('success_patterns', {})
            failure_patterns = task_results.get('failure_patterns', {})
            
            success_loc = success_patterns.get('structural_metrics', {}).get('locality', {}).get('average_distance', 0)
            failure_loc = failure_patterns.get('structural_metrics', {}).get('locality', {}).get('average_distance', 0)
            
            if success_loc > 0 or failure_loc > 0:
                success_localities.append(success_loc)
                failure_localities.append(failure_loc)
                task_names.append(task_name.replace('google_robot_', ''))
        
        if task_names:
            x = np.arange(len(task_names))
            width = 0.35
            
            axes[0, 1].bar(x - width/2, success_localities, width, label='Success', alpha=0.7, color='green')
            axes[0, 1].bar(x + width/2, failure_localities, width, label='Failure', alpha=0.7, color='red')
            axes[0, 1].set_xlabel('Tasks')
            axes[0, 1].set_ylabel('Average Distance')
            axes[0, 1].set_title('Locality Comparison Across Tasks')
            axes[0, 1].set_xticks(x)
            axes[0, 1].set_xticklabels(task_names, rotation=45, ha='right')
            axes[0, 1].legend()
        
        # 3. å¤æ‚åº¦å¯¹æ¯”
        success_complexities = []
        failure_complexities = []
        
        for task_name, task_results in all_results.items():
            success_patterns = task_results.get('success_patterns', {})
            failure_patterns = task_results.get('failure_patterns', {})
            
            success_comp = success_patterns.get('structural_metrics', {}).get('complexity', {}).get('effective_rank', 0)
            failure_comp = failure_patterns.get('structural_metrics', {}).get('complexity', {}).get('effective_rank', 0)
            
            if success_comp > 0 or failure_comp > 0:
                success_complexities.append(success_comp)
                failure_complexities.append(failure_comp)
        
        if success_complexities:
            axes[1, 0].scatter(success_complexities, failure_complexities, alpha=0.7, s=100)
            axes[1, 0].set_xlabel('Success Complexity')
            axes[1, 0].set_ylabel('Failure Complexity')
            axes[1, 0].set_title('Success vs Failure Complexity')
            
            # æ·»åŠ å¯¹è§’çº¿
            max_val = max(max(success_complexities), max(failure_complexities))
            axes[1, 0].plot([0, max_val], [0, max_val], 'k--', alpha=0.5)
            axes[1, 0].grid(True, alpha=0.3)
        
        # 4. åˆ†æä»»åŠ¡æ•°é‡ç»Ÿè®¡
        analysis_stats = {
            'Total Tasks': len(all_results),
            'With Success Data': sum(1 for r in all_results.values() if r.get('success_patterns')),
            'With Failure Data': sum(1 for r in all_results.values() if r.get('failure_patterns')),
            'Complete Analysis': sum(1 for r in all_results.values() if r.get('comparison'))
        }
        
        categories = list(analysis_stats.keys())
        values = list(analysis_stats.values())
        
        axes[1, 1].bar(categories, values, alpha=0.7, color=['blue', 'green', 'red', 'purple'])
        axes[1, 1].set_title('Analysis Coverage Statistics')
        axes[1, 1].set_ylabel('Count')
        
        for i, v in enumerate(values):
            axes[1, 1].text(i, v + max(values)*0.01, str(v), ha='center', va='bottom')
        
        plt.tight_layout()
        save_path = os.path.join(self.analysis_dir, "cross_task_summary_charts.png")
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ§  é«˜çº§æ³¨æ„åŠ›æ¨¡å¼åˆ†æå™¨")
    print("=" * 80)
    
    # è·å–è„šæœ¬ç›®å½• - ä¿®æ­£è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = AttentionPatternAnalyzer(current_dir)
    
    # åˆ†æå·²æœ‰ç»“æœ
    analyzer.analyze_existing_results(current_dir)


if __name__ == "__main__":
    main() 